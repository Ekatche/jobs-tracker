import asyncio
import os
import tempfile
import json
import logging
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.async_configs import BrowserConfig, CacheMode
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.content_filter_strategy import LLMContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

logger = logging.getLogger(__name__)


class JobOffer(BaseModel):
    id: int
    poste: str
    entreprise: str
    localisation: str
    date: str
    url: str


# ‚úÖ Configurations globales r√©utilisables
_browser_config = None
_crawl_config = None
_api_key_cache = None


def get_shared_browser_config() -> BrowserConfig:
    """Configuration navigateur partag√©e et r√©utilisable"""
    global _browser_config

    if _browser_config is None:
        # logger.info("üåê Cr√©ation configuration navigateur partag√©e")

        user_data_dir = tempfile.mkdtemp()
        _browser_config = BrowserConfig(
            headless=True,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport_width=1920,
            viewport_height=1080,
            verbose=False,
            user_data_dir=user_data_dir,
        )
        # logger.debug("‚úÖ Configuration navigateur partag√©e cr√©√©e")

    return _browser_config


def get_shared_crawl_config(api_key: str) -> CrawlerRunConfig:
    """Configuration crawl partag√©e et r√©utilisable"""
    global _crawl_config, _api_key_cache

    # ‚úÖ R√©utiliser la config si m√™me cl√© API
    if _crawl_config is None or _api_key_cache != api_key:
        # logger.info("‚öôÔ∏è Cr√©ation configuration crawl partag√©e")

        _api_key_cache = api_key

        # Content filter
        content_filter = LLMContentFilter(
            llm_config=LLMConfig(provider="openai/gpt-4o-mini", api_token=api_key),
            instruction="""
Concentre-toi sur l'extraction des blocs HTML contenant des informations d'offres d'emploi :
- Titre de poste
- Nom de l'entreprise
- Les informations sur le poste (si disponibles)
- Description du poste (si disponible)
- Localisation
- Date ou lien de l'offre

Ignore et supprime tout le reste (menus, filtres, publicit√©s, suggestions, pied de page, etc.).
Retourne uniquement le HTML minimal des offres d√©tect√©es.
""",
            chunk_token_threshold=4096,
            verbose=False,
        )

        # Markdown generator
        md_generator = DefaultMarkdownGenerator(
            content_filter=content_filter,
            options={
                "ignore_links": False,
                "body_width": 0,
                "unicode_snob": True,
                "escape_all": False,
            },
        )

        # Extraction strategy
        extraction_strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(provider="openai/gpt-4o-mini", api_token=api_key),
            schema=json.dumps(JobOffer.model_json_schema()),
            extraction_type="schema",
            instruction="""
Extrait toutes les offres d'emploi de cette page web.
Pour chaque offre, identifie et extrait :
- id : g√©n√®re un ID unique (num√©ro s√©quentiel)
- poste : le titre du poste/m√©tier
- entreprise : nom de l'entreprise qui recrute
- localisation : ville, r√©gion ou lieu de travail
- date : date de publication ou depuis quand l'offre est en ligne
- url : lien vers l'offre compl√®te (si disponible)
Ignore tout contenu qui n'est pas une offre d'emploi (menus, publicit√©s, etc.).
Si une information manque, utilise "Non sp√©cifi√©" ou "" selon le cas.
Retourne une liste d'offres au format JSON.
""",
            Verbose=False,
            apply_chunking=True,
            input_format="markdown",
            extra_args={
                "temperature": 0.1,
                "max_tokens": 4000,
            },
        )

        _crawl_config = CrawlerRunConfig(
            word_count_threshold=50,
            cache_mode=CacheMode.BYPASS,
            screenshot=False,
            verbose=False,
            stream=True,
            locale="fr-FR",
            timezone_id="Europe/Paris",
            prettiify=True,
            remove_overlay_elements=True,
            extraction_strategy=extraction_strategy,
            markdown_generator=md_generator,
        )

        # logger.debug("‚úÖ Configuration crawl partag√©e cr√©√©e")

    return _crawl_config


async def crawl_single_job_url_optimized(
    url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig
) -> Dict[str, Any]:
    """Version optimis√©e qui r√©utilise un crawler et une config existants"""
    # logger.debug(f"üï∑Ô∏è Crawl optimis√© de: {url}")

    try:
        result = await crawler.arun(url=url, config=config)

        logger.debug(f"üìä Crawl termin√© - Success: {result.success}")

        if result.success and result.extracted_content:
            logger.debug("‚úÖ Contenu extrait, parsing JSON...")

            try:
                offers = json.loads(result.extracted_content)

                if isinstance(offers, dict):
                    offers = [offers]
                elif not isinstance(offers, list):
                    offers = []

                logger.info(f"üéØ {len(offers)} offres extraites de {url}")

                return {
                    "url": url,
                    "status": "success",
                    "offers_count": len(offers),
                    "offers": offers,
                }

            except json.JSONDecodeError as e:
                logger.error(f"‚ùå Erreur JSON pour {url}: {e}")
                return {
                    "url": url,
                    "status": "json_error",
                    "error": str(e),
                    "raw_content": result.extracted_content[:200] + "...",
                }
        else:
            error_msg = result.error_message or "Aucun contenu extrait"
            logger.warning(f"‚ö†Ô∏è Crawl de {url} sans succ√®s: {error_msg}")
            return {
                "url": url,
                "status": "failed",
                "error": error_msg,
            }

    except Exception as e:
        logger.error(f"üí• Exception lors du crawl de {url}: {e}")
        return {"url": url, "status": "exception", "error": str(e)}


# =====================================================================
# ======== Crawl wensite and get fit markdown result ==================
# =====================================================================


async def get_filtered_markdown(
    url: str, api_key: Optional[str] = None, include_raw_markdown: bool = False
) -> Dict[str, Any]:
    """
    R√©cup√®re le markdown filtr√© d'une URL avec le filtre LLM

    Args:
        url: URL √† crawler
        api_key: Cl√© API OpenAI (optionnelle)
        include_raw_markdown: Inclure aussi le markdown non filtr√©

    Returns:
        Dict contenant le markdown filtr√© et les m√©tadonn√©es
    """
    # logger.info(f"üìÑ R√©cup√©ration markdown filtr√© pour: {url}")

    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY manquante")

    try:
        # ‚úÖ Configurations
        browser_config = get_shared_browser_config()

        # ‚úÖ Content filter sp√©cialis√© pour le markdown
        content_filter = LLMContentFilter(
            llm_config=LLMConfig(provider="openai/gpt-4o-mini", api_token=api_key),
            instruction="""
            Filtre cette page web pour ne garder que le contenu pertinent li√© aux offres d'emploi.

            GARDE:
            - Titres de postes
            - Noms d'entreprises
            - Informations sur les postes (salaire, type de contrat, etc.)
            - Descriptions de postes
            - Localisations/lieux de travail
            - Dates de publication
            - Liens vers les offres d√©taill√©es
            - Crit√®res de candidature/comp√©tences requises

            SUPPRIME:
            - Menus de navigation
            - Barres lat√©rales
            - Publicit√©s
            - Pied de page
            - Formulaires de recherche/filtres
            - Contenus promotionnels
            - Widgets sociaux
            - Cookies/RGPD banners

            Retourne uniquement le HTML nettoy√© focalis√© sur les offres d'emploi.
            """,
            chunk_token_threshold=6000,  # Plus large pour le markdown
            verbose=False,
        )

        #  Markdown generator avec filtre
        md_generator = DefaultMarkdownGenerator(
            content_filter=content_filter,
            options={
                "ignore_links": False,  # Garder les liens
                "body_width": 0,  # Pas de limite de largeur
                "unicode_snob": True,
                "escape_all": False,
                "mark_code": True,  # Marquer le code
                "wrap_links": False,
                "protect_links": True,
                "strip_whitespace": True,
            },
        )

        # Configuration crawl pour markdown uniquement
        crawl_config = CrawlerRunConfig(
            word_count_threshold=10,
            cache_mode=CacheMode.BYPASS,
            screenshot=False,
            verbose=True,
            locale="fr-FR",
            timezone_id="Europe/Paris",
            prettiify=True,
            remove_overlay_elements=True,
            markdown_generator=md_generator,
        )

        # ‚úÖ Crawler
        async with AsyncWebCrawler(config=browser_config) as crawler:
            logger.debug("üì± Crawler initialis√© pour extraction markdown")
            result = await crawler.arun(url=url, config=crawl_config)
            logger.info(f"üìä Crawl termin√© - Success: {result.success}")

            if result.success:
                filtered_markdown = result.markdown

                # M√©tadonn√©es
                metadata = {
                    "url": url,
                    "title": getattr(result, "title", None),
                    "timestamp": getattr(result, "timestamp", None),
                    "word_count": (
                        len(filtered_markdown.split()) if filtered_markdown else 0
                    ),
                    "char_count": len(filtered_markdown) if filtered_markdown else 0,
                }

                return {
                    "status": "success",
                    "url": url,
                    "filtered_markdown": filtered_markdown,
                    "metadata": metadata,
                }

            else:
                error_msg = result.error_message or "√âchec du crawl"
                logger.error(f"‚ùå Crawl √©chou√© pour {url}: {error_msg}")

                return {
                    "url": url,
                    "status": "failed",
                    "error": error_msg,
                    "fit_markdown": None,
                }
    except Exception as e:
        logger.error(f"üí• Exception lors du crawl markdown de {url}: {e}")

        return {
            "url": url,
            "status": "exception",
            "error": str(e),
            "fit_markdown": None,
        }


# ====================================
# ======== Extract job offers ========
# ====================================


async def crawl_and_extract_jobs_optimized(
    urls: List[str],
    api_key: Optional[str] = None,
    max_concurrent: int = 3,
    filter_keywords: Optional[List[str]] = None,
    filter_locations: Optional[List[str]] = None,
    filter_companies: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Version optimis√©e qui r√©utilise les configurations"""

    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY manquante")

    try:
        #  Cr√©er les configurations une seule fois
        browser_config = get_shared_browser_config()
        crawl_config = get_shared_crawl_config(api_key)

        logger.info("üì∂ Lancement crawl optimis√©...")

        #  Un seul crawler partag√© pour toutes les URLs
        async with AsyncWebCrawler(config=browser_config) as crawler:
            # logger.debug("üì± Crawler unique initialis√©")

            #  Contr√¥le de concurrence
            semaphore = asyncio.Semaphore(max_concurrent)

            async def crawl_with_semaphore(url: str):
                async with semaphore:
                    return await crawl_single_job_url_optimized(
                        url, crawler, crawl_config
                    )

            #  Ex√©cution parall√®le avec crawler partag√©
            tasks = [crawl_with_semaphore(url) for url in urls]
            crawl_results = await asyncio.gather(*tasks, return_exceptions=True)

        # Traitement des r√©sultats
        processed_results = []
        all_offers = []

        for i, result in enumerate(crawl_results):
            if isinstance(result, Exception):
                logger.error(f"‚ùå Exception pour URL {urls[i]}: {result}")
                processed_results.append(
                    {"url": urls[i], "status": "exception", "error": str(result)}
                )
            else:
                processed_results.append(result)

                # Extraire les offres
                if result.get("status") == "success" and result.get("offers"):
                    for offer in result["offers"]:
                        offer["source_url"] = result["url"]
                        all_offers.append(offer)

        # ‚úÖ Filtrage si n√©cessaire
        if filter_keywords or filter_locations or filter_companies:
            logger.info("üîç Application des filtres...")
            all_offers = filter_offers(
                all_offers, filter_keywords, filter_locations, filter_companies
            )

        # ‚úÖ R√©sum√©
        summary = {
            "total_urls": len(urls),
            "successful_crawls": sum(
                1 for r in processed_results if r.get("status") == "success"
            ),
            "total_offers": len(all_offers),
        }

        logger.info("üéØ Pipeline optimis√© termin√©:")
        logger.info(f"  üìä URLs: {summary['total_urls']}")
        logger.info(f"  ‚úÖ Succ√®s: {summary['successful_crawls']}")
        logger.info(f"  üìã Offres: {summary['total_offers']}")

        return {
            "crawl_results": processed_results,
            "offers": all_offers,
            "summary": summary,
        }

    except Exception as e:
        logger.error(f"üí• Erreur pipeline optimis√©: {e}")
        raise


def filter_offers(
    offers: List[Dict[str, Any]],
    keywords: Optional[List[str]] = None,
    locations: Optional[List[str]] = None,
    companies: Optional[List[str]] = None,
) -> List[Dict[str, Any]]:
    """Filtre les offres selon des crit√®res"""
    if not any([keywords, locations, companies]):
        return offers

    logger.info(f"üîç Filtrage de {len(offers)} offres")

    filtered_offers = offers.copy()

    if keywords:
        keywords_lower = [k.lower() for k in keywords]
        filtered_offers = [
            offer
            for offer in filtered_offers
            if any(
                keyword in offer.get("poste", "").lower() for keyword in keywords_lower
            )
        ]
        logger.debug(f"üìã Apr√®s filtrage mots-cl√©s: {len(filtered_offers)} offres")

    if locations:
        locations_lower = [loc.lower() for loc in locations]
        filtered_offers = [
            offer
            for offer in filtered_offers
            if any(
                location in offer.get("localisation", "").lower()
                for location in locations_lower
            )
        ]
        logger.debug(f"üìç Apr√®s filtrage localisation: {len(filtered_offers)} offres")

    if companies:
        companies_lower = [c.lower() for c in companies]
        filtered_offers = [
            offer
            for offer in filtered_offers
            if any(
                company in offer.get("entreprise", "").lower()
                for company in companies_lower
            )
        ]
        logger.debug(f"üè¢ Apr√®s filtrage entreprises: {len(filtered_offers)} offres")

    logger.info(f"‚úÖ Filtrage termin√©: {len(offers)} -> {len(filtered_offers)} offres")
    return filtered_offers


#  Fonction de nettoyage pour lib√©rer les ressources
def cleanup_shared_configs():
    """Nettoie les configurations partag√©es"""
    global _browser_config, _crawl_config, _api_key_cache
    _browser_config = None
    _crawl_config = None
    _api_key_cache = None
    logger.info("üßπ Configurations partag√©es nettoy√©es")
